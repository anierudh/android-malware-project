{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding,LSTM,Dropout\n",
    "from keras.layers.convolutional import Conv1D,MaxPooling1D\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_auc_score,roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting XML data and save it in a list and assigning labels to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd=os.getcwd()\n",
    "path1=cwd + '/'+ 'data'\n",
    "train_data=os.path.join(path1+'/'+'train,val,test')\n",
    "\n",
    "xml_data=[]\n",
    "labels = []\n",
    "for label_type in ['benign' , 'malicious']:\n",
    "    dir_name=os.path.join(train_data,label_type)\n",
    "    \n",
    "    for fname in os.listdir(dir_name):\n",
    "        dat=os.path.join(dir_name,fname)\n",
    "        f= open(dat,'r').read().split(',')\n",
    "        xml_data.append(f[0])\n",
    "        \n",
    "        if label_type == 'benign':\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the list data's to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type (xml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "<manifest android:versionCode=\"2\" android:versionName=\"1.1\" package=\"com.timalyzer.trial\"\n",
      "  xmlns:android=\"http://schemas.android.com/apk/res/android\">\n",
      "    <uses-permission android:name=\"android.permission.READ_CONTACTS\" />\n",
      "    <uses-permission android:name=\"android.permission.READ_SMS\" />\n",
      "    <uses-permission android:name=\"android.permission.INTERNET\" />\n",
      "    <application android:label=\"@string/app_name\" android:icon=\"@drawable/timalyzer\">\n",
      "        <activity android:label=\"@string/app_name\" android:name=\"TimalyzerActivity\">\n",
      "            <intent-filter>\n",
      "                <action android:name=\"android.intent.action.MAIN\" />\n",
      "                <category android:name=\"android.intent.category.LAUNCHER\" />\n",
      "            </intent-filter>\n",
      "        </activity>\n",
      "        <activity android:name=\"TimalyzerPreferenceActivity\" />\n",
      "    </application>\n",
      "</manifest>\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print (xml_data[0])\n",
    "print (labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406\n",
      "406\n"
     ]
    }
   ],
   "source": [
    "print (len(xml_data))\n",
    "print (len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting and Tokenizing the xml data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2808 unique tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer=Tokenizer(num_words=None,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', \\\n",
    "         lower=True, split=' ', char_level=False, oov_token=None)\n",
    "\n",
    "tokenizer.fit_on_texts(xml_data)\n",
    "sequences=tokenizer.texts_to_sequences(xml_data)\n",
    "word_index=tokenizer.word_index\n",
    "print ('found %s unique tokens' %(len(word_index)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2150\n"
     ]
    }
   ],
   "source": [
    "q=[]\n",
    "for i in sequences:\n",
    "    s=len(i)\n",
    "    q.append(s)\n",
    "print (max(q))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As benign and malicious data is continous we are shuffling before making train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pad_sequences(sequences,maxlen=2150)\n",
    "labels=np.array(labels)\n",
    "xml_data=np.array(xml_data)\n",
    "indices=np.arange(data.shape[0])\n",
    "indices1=np.arange(labels.shape[0])\n",
    "#In future use sklearn train test split \n",
    "np.random.shuffle(indices)\n",
    "np.random.shuffle(indices1)\n",
    "\n",
    "data=data[indices]\n",
    "labels=labels[indices1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(data,labels,test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2150, 5)           14045     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               68608     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 82,782\n",
      "Trainable params: 82,782\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(2809,5,input_length=2150))\n",
    "#model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(128,dropout=0.2, recurrent_dropout=0.2))\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2809, 5)\n",
      "(5, 512)\n",
      "(128, 512)\n",
      "(512,)\n",
      "(128, 1)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "for weight in model.get_weights():\n",
    "    print (weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 4.8309218e-02,  5.5364743e-03,  5.8081038e-03, -4.8358217e-03,\n",
       "          2.4013545e-02],\n",
       "        [ 9.2744716e-03, -2.4890935e-02, -2.1865595e-02, -2.1379029e-02,\n",
       "         -2.8614117e-02],\n",
       "        [ 3.3279989e-02,  4.4077206e-02,  7.0122965e-03,  1.3881531e-02,\n",
       "          1.6763058e-02],\n",
       "        ...,\n",
       "        [ 4.3794740e-02, -4.4476654e-02,  4.6426877e-03, -4.5676004e-02,\n",
       "         -5.5567399e-03],\n",
       "        [ 1.5836690e-02,  1.5009854e-02,  8.7976083e-03, -1.3246369e-02,\n",
       "          3.7222393e-03],\n",
       "        [-7.8093261e-05,  6.4395443e-03, -4.3376591e-02,  3.7731979e-02,\n",
       "         -4.5094814e-02]], dtype=float32)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=model.layers[0].get_weights()[0]\n",
    "words_embeddings = {w:embeddings[idx] for w, idx in word_index.items()}\n",
    "a = np.array ([words_embeddings])\n",
    "np.savetxt(\"5d.txt\",a,fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=model.layers[0].get_weights()[0]\n",
    "words_embeddings = {w:embeddings[idx] for w, idx in word_index.items()}\n",
    "tokens=[]\n",
    "lab=[]\n",
    "for word in words_embeddings:\n",
    "        tokens.append(words_embeddings[word])\n",
    "        lab.append(word)\n",
    "x=TSNE()\n",
    "vis=x.fit_transform(tokens[:100])\n",
    "xim = []\n",
    "y = []\n",
    "for value in vis:\n",
    "    xim.append(value[0])\n",
    "    y.append(value[1])\n",
    "        \n",
    "plt.figure(figsize=(16, 16)) \n",
    "for i in range(len(xim)):\n",
    "    plt.scatter(xim[i],y[i])\n",
    "    plt.annotate(lab[i],xy=(xim[i], y[i]),xytext=(5, 2),textcoords='offset points',ha='right',va='bottom')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (words_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test),batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting for test data and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=model.predict(x_test,batch_size=32)\n",
    "for i in range(len(preds)):\n",
    "    if preds[i]<=0.5:\n",
    "        temp=0\n",
    "        preds[i]=temp\n",
    "    else:\n",
    "        preds[i]=1\n",
    "            \n",
    "eva=model.evaluate(x_test,y_test)\n",
    "print (eva)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title(\"Learning curve\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(['train','val'],loc= 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(\"Learning curve\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(['train','val'],loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (classification_report(y_test,preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix=confusion_matrix(y_test,preds)\n",
    "print (cnf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "roc auc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3]",
   "language": "python",
   "name": "conda-env-miniconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
