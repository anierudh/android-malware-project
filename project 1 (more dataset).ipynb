{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding,Dropout,LSTM\n",
    "from keras.layers.convolutional import Conv1D,MaxPooling1D\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting XML data and save it in a list and assigning labels to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd=os.getcwd()\n",
    "path1=cwd + '/'+ 'data'\n",
    "train_data=os.path.join(path1+'/'+'train,val,test')\n",
    "\n",
    "xml_data=[]\n",
    "labels = []\n",
    "for label_type in ['benxml' , 'malxml']:\n",
    "    dir_name=os.path.join(train_data,label_type)\n",
    "    \n",
    "    for fname in os.listdir(dir_name):\n",
    "        dat=os.path.join(dir_name,fname)\n",
    "        f= open(dat,'r').read().split(',')\n",
    "        xml_data.append(f[0])\n",
    "        \n",
    "        if label_type == 'benxml':\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the list data's to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (xml_data[0])\n",
    "print (labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(xml_data))\n",
    "print (len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting and Tokenizing the xml data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=None,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', \\\n",
    "         lower=True, split=' ', char_level=False, oov_token=None)\n",
    "\n",
    "tokenizer.fit_on_texts(xml_data)\n",
    "sequences=tokenizer.texts_to_sequences(xml_data)\n",
    "word_index=tokenizer.word_index\n",
    "print ('found %s unique tokens' %(len(word_index)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k,v in word_index.items():\n",
    "    for s in sequences[0]:\n",
    "        if s==v:\n",
    "            print (s,k)\n",
    "                \n",
    "       \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sequences[5413])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k1,v1 in word_index.items():\n",
    "    for s1 in sequences[5413]:\n",
    "        if s1==v1:\n",
    "            print (s1,k1)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=[]\n",
    "for i in sequences:\n",
    "    s=len(i)\n",
    "    q.append(s)\n",
    "    s=max(q)\n",
    "print (s)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As benign and malicious data is continous we are shuffling before making train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pad_sequences(sequences,maxlen=s)\n",
    "labels=np.array(labels)\n",
    "xml_data=np.array(xml_data)\n",
    "indices=np.arange(data.shape[0])\n",
    "indices1=np.arange(labels.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "np.random.shuffle(indices1)\n",
    "\n",
    "data=data[indices]\n",
    "labels=labels[indices1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making training (60%) and test data (40%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(data,labels,test_size=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(66082,5,input_length=s))\n",
    "model.add(Flatten())\n",
    "#model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=2))\n",
    "#model.add(LSTM(128,dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='RMSprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the shape of all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wei in model.get_weights():\n",
    "    print (wei.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving the embeddings in a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings=model.layers[0].get_weights()[0]\n",
    "#words_embeddings = {w:embeddings[idx] for w, idx in word_index.items()}\n",
    "#a = np.array ([words_embeddings])\n",
    "#np.savetxt(\"5d.txt\",a,fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test),batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=model.layers[0].get_weights()[0]\n",
    "words_embeddings = {w:embeddings[idx] for w, idx in word_index.items()}\n",
    "words_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualization of embeddings first 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings=model.layers[0].get_weights()[0]\n",
    "words_embeddings = {w:embeddings[idx] for w, idx in word_index.items()}\n",
    "tokens=[]\n",
    "lab=[]\n",
    "for word in words_embeddings:\n",
    "        tokens.append(words_embeddings[word])\n",
    "        lab.append(word)\n",
    "x=TSNE(n_iter=1000)\n",
    "vis=x.fit_transform(tokens[:500])\n",
    "xim = []\n",
    "y = []\n",
    "for value in vis:\n",
    "    xim.append(value[0])\n",
    "    y.append(value[1])\n",
    "        \n",
    "plt.figure(figsize=(16, 16)) \n",
    "for i in range(len(xim)):\n",
    "    plt.scatter(xim[i],y[i],c='steelblue',edgecolors='k')\n",
    "    plt.annotate(lab[i],xy=(xim[i], y[i]),xytext=(5, 2),textcoords='offset points',ha='right',va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualization of embeddings for all words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=model.layers[0].get_weights()[0]\n",
    "words_embeddings = {w:embeddings[idx] for w, idx in word_index.items()}\n",
    "tokens=[]\n",
    "lab=[]\n",
    "for word in words_embeddings:\n",
    "        tokens.append(words_embeddings[word])\n",
    "        lab.append(word)\n",
    "x=TSNE(n_iter=1000)\n",
    "vis=x.fit_transform(tokens)\n",
    "xim = []\n",
    "y = []\n",
    "for value in vis:\n",
    "    xim.append(value[0])\n",
    "    y.append(value[1])\n",
    "        \n",
    "plt.figure(figsize=(16, 16)) \n",
    "for i in range(len(xim)):\n",
    "    plt.scatter(xim[i],y[i],c='Red',edgecolors='k')\n",
    "    plt.annotate(lab[i],xy=(xim[i], y[i]),xytext=(5, 2),textcoords='offset points',ha='right',va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the similarity between the vectors using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=model.get_weights()[0]\n",
    "embeddings=embeddings[1:]\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "# compute pairwise distance matrix\n",
    "numerator=embeddings.T @ embeddings\n",
    "\n",
    "dr=(embeddings * embeddings).sum(0, keepdims=True) ** .5\n",
    "\n",
    "similarity=numerator / dr / dr.T\n",
    "\n",
    "\n",
    "\n",
    "distance_matrix = 1-pairwise_distances(similarity,metric='cosine')\n",
    "print (distance_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "key=[]\n",
    "for i in words_embeddings.keys():\n",
    "    key.append(i)\n",
    "for i in range(len(words_embeddings)):\n",
    "    for j in range(len(words_embeddings)):\n",
    "        string=[]\n",
    "        if i==j:\n",
    "            continue\n",
    "        else:\n",
    "            string.append(key[i])\n",
    "            string.append(key[j])\n",
    "            dist=wasserstein_distance(words_embeddings[key[i]],words_embeddings[key[j]])\n",
    "            print ([key[i],key[j]],dist)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final={}\n",
    "for i in range(len(words_embeddings)):\n",
    "    for j in range(len(words_embeddings)):\n",
    "        string=[]\n",
    "\n",
    "        if i==j:\n",
    "            continue\n",
    "        else:\n",
    "            string.append(key[i])\n",
    "            string.append(key[j])\n",
    "            #print (string)\n",
    "            t=tuple(string)\n",
    "            dist=wasserstein_distance(words_embeddings[key[i]],words_embeddings[key[j]])\n",
    "            #print ([dicts[key[i]],dicts[key[j]]],dist)\n",
    "            final[t]=dist\n",
    "print (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# view contextually similar words\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "#words_embeddings = {w:embeddings[idx] for w, idx in word_index.items()}\n",
    "#similar_words = {search_term: [words_embeddings[idx] for idx in distance_matrix[word_index[search_term]-1].argsort()[1:6]+1] \n",
    "#               for search_term in ['wifi', 'permission', 'package', 'write', 'read', 'location', 'phone','android']}\n",
    "\n",
    "#similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting for test data and and giving the criteria for binary classification the values below and equal to 0.5 is class 0 benign and above 0.5 is class 1 malicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=model.predict(x_test,batch_size=32)\n",
    "for i in range(len(preds)):\n",
    "    if preds[i]<=0.5:\n",
    "        temp=0\n",
    "        preds[i]=temp\n",
    "    else:\n",
    "        preds[i]=1\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eva=model.evaluate(x_test,y_test)\n",
    "print (eva)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title(\"Learning curve\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(['train','val'],loc= 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(\"Learning curve\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(['train','val'],loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification report, confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (classification_report(y_test,preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix=confusion_matrix(y_test,preds)\n",
    "print (cnf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "roc auc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test,preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3]",
   "language": "python",
   "name": "conda-env-miniconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
